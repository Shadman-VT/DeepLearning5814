{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fb751b-ca72-4528-8de6-b89bd111c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set the default font family to Times New Roman, default tick label color to black, and default line plot style\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"xtick.color\"] = \"black\"\n",
    "plt.rcParams[\"ytick.color\"] = \"black\"\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.tab10.colors) + plt.cycler(\"marker\", [\"o\", \"s\", \"D\", \"v\", \"X\", \"P\", \">\", \"<\", \"H\", \"d\"])  # Set the default line plot style\n",
    "\n",
    "\n",
    "\n",
    "# Function to create RNN dataset\n",
    "def create_rnn_dataset(X_norm, y_scaled, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_norm)-lookback):\n",
    "        X.append(X_norm[i:(i+lookback)])\n",
    "        y.append(y_scaled[(i+lookback)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Define a function to calculate performance metrics\n",
    "def calculate_performance(observed, predicted):\n",
    "    mse = mean_squared_error(observed, predicted)\n",
    "    mae = mean_absolute_error(observed, predicted)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(observed, predicted)\n",
    "    \n",
    "    # Return a dictionary with the metrics\n",
    "    return float(mse), float(mae), float(rmse), float(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39d16f9-f6a4-47f6-9511-8c021cf44af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is being used\n"
     ]
    }
   ],
   "source": [
    "# Set up GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU is being used')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6536b9-19b4-4992-b68c-e5eb77f8c6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime (GMT)       object\n",
      "HT (m)              float64\n",
      "NTR (m)             float64\n",
      "MSL (Pa)            float64\n",
      "SST (°C)            float64\n",
      "Air_Temp (°C)       float64\n",
      "Wind Speed (m/s)    float64\n",
      "TWL (m)             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_true = pd.read_csv(r\"C:\\Users\\Shadman\\Downloads\\CW_TWL_Merged_Filled-Final_6VariableUpdated_v2.csv\")\n",
    "print(data_true.dtypes)\n",
    "#drop the date column, all data are at 1hr interval. Timestamp has no additional values\n",
    "data = data_true.drop(columns=['DateTime (GMT)']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604fc0cb-5346-4f5b-94c9-54f78cab3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fortmyers latest.csv dataset\n",
    "X = data.iloc[:,:-1]  # All columns excepect the last column (target variable)\n",
    "y = data.iloc[:, -1]  # The target variable\n",
    "\n",
    "# Step 1: Normalize the data using MinMaxScaler\n",
    "# Note: HT (m), NTR (m) can be scaled from -1 to 1 as the magnitude both has negative and positive values\n",
    "# Simialarly Wind Speed (m/s) and Wind Gist (m/s) has been scaled from 0 to 1 because there are no negative values here.\n",
    "# Likewise, the target varibale has been scaled from -1 to 1.\n",
    "######################### HOWEVER #######################################\n",
    "# As scaling HT and NTR from -1 to 1 preserves their positive and negative values, while Wind Speed and Wind Gust are appropriately scaled\n",
    "# from 0 to 1 due to their non-negative nature. Similarly, scaling the target variable from -1 to 1 is beneficial if it has both positive\n",
    "# a nd negative values. However, using different scaling ranges for features may cause some models, like neural networks, to focus\n",
    "# disproportionately on certain variables, potentially slowing convergence. To mitigate this, consider experimenting with\n",
    "# uniform scaling across all features for better consistency, especially in models that are sensitive to feature magnitudes.\n",
    "\n",
    "########################## Therefore, as a start, all input varibales are scaled from -1 to 1 and all target varibales are scaled from -1 to 1.\n",
    "########################## A future trail will be held to see if HT (m), NTR (m) can be scaled from 0 to 1, \n",
    "########################### and Wind Speed (m/s) and Wind Gist (m/s) scaled from 0 to 1 can help to improve the model.\n",
    "########################### so as a start all input variables are scaled from -1 to 1. Including the windspeed and wind gust.\n",
    "\n",
    "##### Therefore, speperate scaling algorithms are set for input and target varibales\n",
    "\n",
    "# Step 1: Initialize MinMaxScaler for X (features) and set range to (-1, 1) \n",
    "# this can be later changed into 0 to 1\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_norm = scaler_X.fit_transform(X)\n",
    "\n",
    "# Initialize MinMaxScaler for y (target) and set range to (-1, 1)\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape y to a 2D array since MinMaxScaler expects a 2D input\n",
    "y_reshaped = y.values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the target variable (y) to scale it between -1 and 1\n",
    "y_scaled = scaler_y.fit_transform(y_reshaped)\n",
    "\n",
    "# Inverse transform the scaled target variable back to the original scale\n",
    "# This is done so that we can calcualte performance metrics\n",
    "y_original = scaler_y.inverse_transform(y_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3888f73f-64b5-418f-9eea-de3293e6d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaN in X_norm after scaling: 0\n",
      "Check for NaN in y_scaled after scaling: 0\n",
      "-----------------------------------------------------------\n",
      "Minimum value of scaled data (X): -1.0\n",
      "Maximum value of scaled data (X): 1.0\n",
      "-----------------------------------------------------------\n",
      "Minimum value of scaled data (y): -1.0\n",
      "Maximum value of scaled data (y): 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in scaled data\n",
    "print(\"Check for NaN in X_norm after scaling:\", np.isnan(X_norm).sum())\n",
    "print(\"Check for NaN in y_scaled after scaling:\", np.isnan(y_scaled).sum())\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "# Print the minimum and maximum values of the scaled data\n",
    "min_value_X = np.min(X_norm)\n",
    "max_value_X = np.max(X_norm)\n",
    "print(\"Minimum value of scaled data (X):\", min_value_X)\n",
    "print(\"Maximum value of scaled data (X):\", max_value_X)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "min_value_y = np.min(y_scaled)\n",
    "max_value_y = np.max(y_scaled)\n",
    "print(\"Minimum value of scaled data (y):\", min_value_y)\n",
    "print(\"Maximum value of scaled data (y):\", max_value_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7961fd-04f5-4d47-908f-392bf7ea255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61369, 6)\n",
      "(61369, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_norm.shape)\n",
    "print(y_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1efeec39-9076-41cd-a795-05b91b26924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split(%):  80.0\n",
      "---------------------------------\n",
      "Number of samples in X_train: 49095\n",
      "Number of samples in X_test: 12274\n",
      "Number of samples in y_train: 49095\n",
      "Number of samples in y_test: 12274\n"
     ]
    }
   ],
   "source": [
    "#Training data \n",
    "data_spilt=0.80\n",
    "train_size = int(0.80 * len(X_norm))\n",
    "print('Data Split(%): ', data_spilt*100)\n",
    "print('---------------------------------')\n",
    "# Split the preprocessed dataset into training and testing sets\n",
    "X_train = X_norm[:train_size]\n",
    "X_test = X_norm[train_size:]\n",
    "y_train = y_scaled[:train_size]\n",
    "y_test = y_scaled[train_size:]\n",
    "\n",
    "# lookback samples. Initially starting at 4 hours, 1/3 of one single tide cycle. Will be updated later 3,4,5,6 .\n",
    "lookback= 12\n",
    "\n",
    "# Define the number of input features\n",
    "num_input_features = X_norm.shape[1]\n",
    "\n",
    "# Define the number of output fea.shape[1]tures\n",
    "num_output_features = y_scaled.shape[1]\n",
    "\n",
    "# Split the preprocessed dataset into training and testing sets\n",
    "X_train, X_test = X_norm[:train_size], X_norm[train_size:]\n",
    "y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]\n",
    "\n",
    "print(\"Number of samples in X_train:\", X_train.shape[0])\n",
    "print(\"Number of samples in X_test:\", X_test.shape[0])\n",
    "print(\"Number of samples in y_train:\", y_train.shape[0])\n",
    "print(\"Number of samples in y_test:\", y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4fe37f-b088-443f-9582-6acc597b1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input data shape for test: (batch_size, timesteps, num_features)\n",
      "Shape of X_train_rnn: (49083, 12, 6)\n",
      "There will be a total of 49089 samples. The first sample will be as following: \n"
     ]
    }
   ],
   "source": [
    "### creating the rnn data set for timeseiries with lookback\n",
    "X_train_rnn, y_train_rnn = create_rnn_dataset(X_train, y_train, lookback)\n",
    "X_test_rnn, y_test_rnn = create_rnn_dataset(X_test, y_test, lookback)\n",
    "\n",
    "print(\"Expected input data shape for test: (batch_size, timesteps, num_features)\")\n",
    "print(\"Shape of X_train_rnn:\", X_train_rnn.shape)\n",
    "print(\"There will be a total of 49089 samples. The first sample will be as following: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b4e82-61da-4d05-a013-21657c01beb9",
   "metadata": {},
   "source": [
    "Sample 1:\n",
    "\n",
    "```python\n",
    "[\n",
    "    [HT_t1, NTR_t1, WindSpeed_t1, WindGust_t1],\n",
    "    [HT_t2, NTR_t2, WindSpeed_t2, WindGust_t2],\n",
    "    [HT_t3, NTR_t3, WindSpeed_t3, WindGust_t3],\n",
    "    [HT_t4, NTR_t4, WindSpeed_t4, WindGust_t4],\n",
    "    [HT_t5, NTR_t5, WindSpeed_t5, WindGust_t5],\n",
    "    [HT_t6, NTR_t6, WindSpeed_t6, WindGust_t6],\n",
    "]\n",
    "\n",
    "Shape of X_train_rnn: (49089, 6, 4)\n",
    "Samples: The model will process each of the 49,089 sequences individually to learn patterns from the historical data.\n",
    "Timesteps: The model considers the past 6 time steps (lookback) for each sequence, learning how these past observations influence the next time step.\n",
    "Features: The 4 features for each time step provide the model with a comprehensive view of the conditions at each point in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e03051e6-b8b8-4885-a483-6cebe546c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Not understanding why you have to transpose here!!!! Again, the model wants this, why is that?\n",
    "# X_train_rnn = X_train_rnn.transpose(0, 2, 1)\n",
    "# X_test_rnn = X_test_rnn.transpose(0, 2, 1)\n",
    "# print(X_train_rnn.shape, X_test_rnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b42dff6d-c6e1-4dba-8e42-d0cf18844bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 12, 960)          1870080   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 960)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 12, 192)          811776    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 192)           0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 896)              2297344   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 896)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 897       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,980,097\n",
      "Trainable params: 2,298,241\n",
      "Non-trainable params: 2,681,856\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "192/192 [==============================] - 6s 14ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 2/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 9.8495e-04 - val_mae: 0.0223\n",
      "Epoch 3/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 0.0011 - val_mae: 0.0237\n",
      "Epoch 4/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 9.5208e-04 - val_mae: 0.0217\n",
      "Epoch 5/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 9.2003e-04 - val_mae: 0.0212\n",
      "Epoch 6/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 0.0010 - val_mae: 0.0229\n",
      "Epoch 7/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 9.5721e-04 - val_mae: 0.0218\n",
      "Epoch 8/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 0.0013 - val_mae: 0.0264\n",
      "Epoch 9/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 9.5729e-04 - val_mae: 0.0217\n",
      "Epoch 10/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0254 - val_loss: 9.1649e-04 - val_mae: 0.0210\n",
      "Epoch 11/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 9.8019e-04 - val_mae: 0.0222\n",
      "Epoch 12/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 9.5558e-04 - val_mae: 0.0217\n",
      "Epoch 13/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 9.5728e-04 - val_mae: 0.0218\n",
      "Epoch 14/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0250 - val_loss: 9.1139e-04 - val_mae: 0.0210\n",
      "Epoch 15/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0250 - val_loss: 9.2384e-04 - val_mae: 0.0212\n",
      "Epoch 16/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 9.0758e-04 - val_mae: 0.0210\n",
      "Epoch 17/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0011 - val_mae: 0.0240\n",
      "Epoch 18/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0011 - val_mae: 0.0232\n",
      "Epoch 19/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 9.3162e-04 - val_mae: 0.0213\n",
      "Epoch 20/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0010 - val_mae: 0.0232\n",
      "Epoch 21/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 9.6384e-04 - val_mae: 0.0220\n",
      "Epoch 22/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 9.1822e-04 - val_mae: 0.0211\n",
      "Epoch 23/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 9.3311e-04 - val_mae: 0.0215\n",
      "Epoch 24/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 9.3842e-04 - val_mae: 0.0216\n",
      "Epoch 25/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 9.4850e-04 - val_mae: 0.0217\n",
      "Epoch 26/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 9.8015e-04 - val_mae: 0.0223\n",
      "Epoch 27/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 9.0833e-04 - val_mae: 0.0211\n",
      "Epoch 28/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 9.6424e-04 - val_mae: 0.0220\n",
      "Epoch 29/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 9.4413e-04 - val_mae: 0.0217\n",
      "Epoch 30/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 8.9203e-04 - val_mae: 0.0208\n",
      "Epoch 31/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 9.7047e-04 - val_mae: 0.0221\n",
      "Epoch 32/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 9.4004e-04 - val_mae: 0.0215\n",
      "Epoch 33/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 9.3276e-04 - val_mae: 0.0216\n",
      "Epoch 34/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 9.3199e-04 - val_mae: 0.0215\n",
      "Epoch 35/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 9.8325e-04 - val_mae: 0.0224\n",
      "Epoch 36/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 9.9484e-04 - val_mae: 0.0225\n",
      "Epoch 37/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 8.9508e-04 - val_mae: 0.0209\n",
      "Epoch 38/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 9.3465e-04 - val_mae: 0.0215\n",
      "Epoch 39/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 0.0010 - val_mae: 0.0226\n",
      "Epoch 40/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 9.2216e-04 - val_mae: 0.0214\n",
      "Epoch 41/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 8.8622e-04 - val_mae: 0.0208\n",
      "Epoch 42/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 9.6423e-04 - val_mae: 0.0221\n",
      "Epoch 43/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 8.9420e-04 - val_mae: 0.0209\n",
      "Epoch 44/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 8.9996e-04 - val_mae: 0.0210\n",
      "Epoch 45/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 9.1693e-04 - val_mae: 0.0213\n",
      "Epoch 46/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 9.2564e-04 - val_mae: 0.0215\n",
      "Epoch 47/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 9.4553e-04 - val_mae: 0.0217\n",
      "Epoch 48/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 9.2224e-04 - val_mae: 0.0214\n",
      "Epoch 49/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 9.1151e-04 - val_mae: 0.0212\n",
      "Epoch 50/50\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 9.0809e-04 - val_mae: 0.0213\n",
      "384/384 [==============================] - 1s 3ms/step - loss: 9.0803e-04 - mae: 0.0213\n",
      "Test Loss: 0.0009080295567400753, Test MAE: 0.02134307660162449\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained BiLSTM model\n",
    "base_model = load_model('G:/My Drive/Deflt3D FM Codes - Vtech/ML+DL_Project/DL-MidTerm/DL_FinalModels/BiLSTM_Model_Hypertuned.h5')\n",
    "\n",
    "# Number of layers to freeze\n",
    "num_layers_freeze = 3\n",
    "\n",
    "# Freeze the first `num_layers_freeze` layers\n",
    "for layer in base_model.layers[:num_layers_freeze]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print model summary to verify\n",
    "base_model.summary()\n",
    "\n",
    "# Compile the model (no new layers are added)\n",
    "base_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the transfer learning model (training the unfrozen layers of the original model)\n",
    "history = base_model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=256, validation_data=(X_test_rnn, y_test_rnn))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mae = base_model.evaluate(X_test_rnn, y_test_rnn)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')\n",
    "\n",
    "# Save the transfer learning model to an h5 file\n",
    "base_model.save('models/hypertuned/TransferLearning_BiLSTM_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef50b66-69b8-4b75-8234-492fbe894e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04a2b3-8a61-4e53-b407-b8ef7dcafef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac990a-e8ac-465d-bba8-95d18e721258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b166640-21c5-43f1-8e78-06a35e7ae5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80dd8d-5b10-4fa1-ba5b-2bf286b998aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dab5a5-dd1c-41fc-b942-7ddf2d55a47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349dafcd-65fb-4289-b906-e7a3e2d74485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801fd31-9bf2-40e7-9cb4-4e28ba3fd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4e7c3-e067-437c-862f-8579983a8da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae367b-567d-4e83-b77b-ad89c6fe016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e82ea-5356-422d-9595-f39f94f1ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3485a49-d985-45ec-90b8-70071be09e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251f42d-5384-4d6b-a37b-f4de39621f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eae541-25ac-479b-826b-0372c7045e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
